import pytest

from src.lib.text import normalize, tokenize, count_freq, top_n


@pytest.mark.parametrize(
    "source, expected",
    [
        ("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
        ("—ë–∂–∏–∫, –Å–ª–∫–∞", "–µ–∂–∏–∫, –µ–ª–∫–∞"),
        ("Hello\r\nWorld", "hello world"),
        ("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"),
        ("", ""),
    ],
)
def test_normalize_basic(source: str, expected: str) -> None:
    """–ë–∞–∑–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏."""
    assert normalize(source) == expected


def test_normalize_yo2e_flag() -> None:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ yo2e ‚Äî –∑–∞–º–µ–Ω–∞ —ë/–Å –Ω–∞ –µ/–ï."""
    text = "—ë–∂–∏–∫ —ë–ª–∫–∞"
    # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é yo2e=True ‚Üí –±—É–∫–≤–∞ "—ë" –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ "–µ"
    assert normalize(text) == "–µ–∂–∏–∫ –µ–ª–∫–∞"
    assert normalize(text, yo2e=True) == "–µ–∂–∏–∫ –µ–ª–∫–∞"
    # –µ—Å–ª–∏ yo2e –≤—ã–∫–ª—é—á–µ–Ω ‚Äî –±—É–∫–≤–∞ "—ë" —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è
    assert normalize(text, yo2e=False) == "—ë–∂–∏–∫ —ë–ª–∫–∞"


@pytest.mark.parametrize(
    "source, expected_tokens",
    [
        ("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]),
        ("hello.txt,world!!!", ["hello", "txt", "world"]),
        ("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]),
        ("2025 –≥–æ–¥", ["2025", "–≥–æ–¥"]),
        ("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ", ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]),
        ("", []),
    ],
)
def test_tokenize_basic(source: str, expected_tokens: list[str]) -> None:
    """–¢–µ—Å—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–ø–æ—Å–ª–µ normalize)."""
    tokens = tokenize(normalize(source))
    assert tokens == expected_tokens


def test_count_freq_and_top_n() -> None:
    """–°–≤—è–∑–∫–∞ count_freq + top_n –Ω–∞ –±–∞–∑–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ –∏ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö."""
    tokens = ["a", "b", "a", "c", "b", "a"]
    freq = count_freq(tokens)

    # —á–∞—Å—Ç–æ—Ç—ã —Å—á–∏—Ç–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
    assert freq == {"a": 3, "b": 2, "c": 1}

    # top_n —Å n=2
    top2 = top_n(freq, n=2)
    assert top2 == [("a", 3), ("b", 2)]

    # –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å ‚Üí –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
    assert top_n({}, n=5) == []


def test_top_n_tie_breaker() -> None:
    """
    –ü—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —á–∞—Å—Ç–æ—Ç–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã —Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É.
    """
    tokens = ["bb", "aa", "bb", "aa", "cc"]
    freq = count_freq(tokens)

    # "aa" –∏ "bb" –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –ø–æ 2 —Ä–∞–∑–∞,
    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã, –ø–æ—Ç–æ–º –ø–æ —Å–ª–æ–≤—É
    top2 = top_n(freq, n=2)
    assert top2 == [("aa", 2), ("bb", 2)]

    # –µ—Å–ª–∏ n –±–æ–ª—å—à–µ —á–∏—Å–ª–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ
    top_all = top_n(freq, n=10)
    assert top_all == [("aa", 2), ("bb", 2), ("cc", 1)]